{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marikhomeriki/Story/blob/master/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqNDta-xZ2u4"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp81GSFN7uHY"
      },
      "source": [
        "* ü§Ø Tech companies and university labs have more computational resources than we do\n",
        "* üòé Let them train their super complex models on millions of images, and then re-use their kernels for our own CNNs!\n",
        "\n",
        "üéØ **<u>Goal:</u>**\n",
        "* ‚òÑÔ∏è Use a **Pretrained Neural Network** $ \\Leftrightarrow $ **Transfer learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR9-yIauZ2vA"
      },
      "source": [
        "## Google Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSm6iw9HizE"
      },
      "source": [
        "Repeat the same process from the last challenge to upload your challenge folder and open your notebook:\n",
        "\n",
        "1. access your [Google Drive](https://drive.google.com/)\n",
        "2. go into the Colab Notebooks folder\n",
        "3. drag and drop this challenge's folder into it\n",
        "4. right-click the notebook file and select `Open with` $\\rightarrow$ `Google Colaboratory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YfeVhhBZ2vC"
      },
      "source": [
        "Don't forget to enable GPU acceleration!\n",
        "\n",
        "`Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator` $\\rightarrow$ `GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoRPmZtQZ2vE"
      },
      "source": [
        "When this is done, run the cells below and get to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q8uOTzh5vwJ",
        "outputId": "432fb037-018e-49fa-cd01-86d9c036cf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7HCO04lsIrcd"
      },
      "outputs": [],
      "source": [
        "# Put Colab in the context of this challenge\n",
        "import os\n",
        "\n",
        "# os.chdir allows you to change directories, like cd in the Terminal\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/data-transfer-learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MdAwhGJdSR"
      },
      "source": [
        "You are now good to go, proceed with the challenge! Don't forget to copy everything back to your PC to upload to Kitt üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdd1Jv7Z2vJ"
      },
      "source": [
        "## (1) What is a Pre-Trained Neural Network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shgjzVW4Z2vJ"
      },
      "source": [
        "* Convolutions are mathematical operations designed to detect specific patterns in input images and use them to classify the images. \n",
        "* One could imagine that these patterns are not 100% specific to one task but to the input images. \n",
        "\n",
        "üöÄ **Why not re-use these kernels - whose weights have already been optimized - somewhere else?** \n",
        "- The expectation is that the trained kernels could also help us perform another classification task.\n",
        "- We are trying to ***transfer*** the knowledge of a trained CNN to a new classification task.\n",
        "\n",
        "\n",
        "üí™ Transfer Learning has two main advantages:\n",
        "- It takes less time to train a pre-trained model since we are not going to update all the weights but only some of them\n",
        "- You benefit from state-of-the-art architectures that have been trained on complex images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOkQIQj4Z2vJ"
      },
      "source": [
        "## (2) Introduction to  VGG16 \n",
        "\n",
        "üìö ***Reading Section, no code***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAkNOgtqF7S_"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, we will use the <a href=\"https://neurohive.io/en/popular-networks/vgg16/\">**`VGG-16 Neural Network`**</a>.\n",
        "\n",
        "> VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper ‚ÄúVery Deep Convolutional Networks for Large-Scale Image Recognition‚Äù. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3√ó3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU‚Äôs.\n",
        "\n",
        "VGG16 is a well-known architecture that has been trained on the <a href=\"https://www.image-net.org/\">**`ImageNet dataset`**</a> which is a very large database of images which belong to different categories. \n",
        "\n",
        "üëâ This architecture already learned which kernels are the best for extracting features from the images found in the `ImageNet dataset`.\n",
        "\n",
        "üëâ As you can see in the illustration, the VGG16 involves millions of parameters you don't want to retrain yourself.\n",
        "\n",
        "\n",
        "<center><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=400></center>\n",
        "\n",
        "‚ùì How does it work in practice ‚ùì\n",
        "\n",
        "* The first layers are not specialized for the particular task the VGG16 CNN was trained on\n",
        "* Only the last dense layer is a \"classification layers\" that can be preceded with a couple of dense layers...  Therefore, we will: \n",
        "    1. Load the existing VGG16 network\n",
        "    2. Remove the last fully connected layers\n",
        "    3. Replace them with some new fully-connected layers (whose weights are randomly set)\n",
        "    4. Train these last layers on a specific classification task. \n",
        "\n",
        "üòÉ Your role is to train only the last layers for your particular problem.\n",
        "\n",
        "ü§ì We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16\">**`tensorflow.keras.applications.VGG16`**</a>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-S858KRF7TA"
      },
      "source": [
        "## (3) Data loading & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzLvo4N5Z2vL"
      },
      "source": [
        "You have two options to load the data into Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6vPgeK2Z2vL"
      },
      "source": [
        "### (Option 1) Loading the data directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6FxEU-HZ2vL"
      },
      "source": [
        "* You can first get the data onto google Colab thanks to:\n",
        "\n",
        "`!wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`,\n",
        "\n",
        "* and then run \n",
        "\n",
        "`!unzip flowers-dataset.zip`\n",
        "\n",
        "*This is a very easy option to load the data into your working directory.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID413NdhZ2vL"
      },
      "source": [
        "### (Option 2) Adding the data to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsnMYzYgZ2vM"
      },
      "source": [
        "* You can first download the data from `https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`. \n",
        "* Then you have to add it to your Google Drive in a folder called `Deep_learning_data` (for instance)\n",
        "* And run the following code in the notebook: \n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "```\n",
        "\n",
        "* The previous code will ask you to go to a given webpage where you can copy a temporary key\n",
        "* Paste it in the cell that will appear in your Colab Notebook\n",
        "* You can now load the data on your Google Colab Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrUUBluZ2vM"
      },
      "source": [
        "### Option 1 or Option 2 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdS5hErJF7TB"
      },
      "source": [
        "* Why choosing option 2 over the option 1? \n",
        "    * ‚úÖ The combo Colab + Drive can be interesting if you work within a project team, and need to update the data from time to time. \n",
        "    * ‚úÖ By doing this, you can share the same data folder with your teammates, and be sure that everyone has the same dataset at any time, even though someone changes it. \n",
        "    * ‚ùå Google Colab has now access to your Google Folder..., which you may or may not be in favor of, depending on your sensibilities...\n",
        "\n",
        "---\n",
        "\n",
        "‚ùì **Question: Loading your dataset** ‚ùì \n",
        "    \n",
        "Use one of the above methods to load your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKw_1TjOF7TC"
      },
      "outputs": [],
      "source": [
        "option_1 = True # Choose here\n",
        "\n",
        "if option_1:\n",
        "    !wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip\n",
        "    !unzip flowers-dataset.zip\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GojliSuJ6Zo7",
        "outputId": "2140eb3a-8c70-489e-b491-c0ddf2bfd7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/data-transfer-learning\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o91sWUAq6Zo7",
        "outputId": "efc1ea77-f680-4844-857e-4d7f02bafb29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flowers\t\t     flowers-dataset.zip.1  transfer_learning.ipynb\n",
            "flowers-dataset.zip  README.md\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onPwIzQzF7TE"
      },
      "source": [
        "‚ùì **Question:Train/Val/Test split** ‚ùì \n",
        "\n",
        "Use the following method to create \n",
        "`X_train, y_train, X_val, y_val, X_test, y_test, num_classes` depending on the `loading_method` you have used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FuhgipEW6Zo7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def load_flowers_data(loading_method):\n",
        "    if loading_method == 'colab':\n",
        "        data_path = '/content/drive/My Drive//Colab Notebooks/data-transfer-learning/flowers'\n",
        "    elif loading_method == 'direct':\n",
        "        data_path = 'flowers/'\n",
        "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for (cl, i) in classes.items():\n",
        "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
        "        for img in tqdm(images_path[:300]):\n",
        "            path = os.path.join(data_path, cl, img)\n",
        "            if os.path.exists(path):\n",
        "                image = Image.open(path)\n",
        "                image = image.resize((256, 256))\n",
        "                imgs.append(np.array(image))\n",
        "                labels.append(i)\n",
        "\n",
        "    X = np.array(imgs)\n",
        "    num_classes = len(set(labels))\n",
        "    y = to_categorical(labels, num_classes)\n",
        "\n",
        "    # Finally we shuffle:\n",
        "    p = np.random.permutation(len(X))\n",
        "    X, y = X[p], y[p]\n",
        "\n",
        "    first_split = int(len(imgs) /6.)\n",
        "    second_split = first_split + int(len(imgs) * 0.2)\n",
        "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
        "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3B1rsIc6Zo8",
        "outputId": "1f61d9f3-333a-4215-88d5-f17d4388d9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:01<00:00, 166.23it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:01<00:00, 152.22it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:01<00:00, 157.07it/s]\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test, num_classes = load_flowers_data('colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm2IsMmGF7TH"
      },
      "source": [
        "‚ùì **Question: Exploring the images** ‚ùì\n",
        "\n",
        "Check the images' shapes and plot a few of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdMfP6dyF7TJ"
      },
      "source": [
        "## (4) A CNN architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njYcWjw1F7TJ"
      },
      "source": [
        "First, let's build our own CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wneAeLYdF7TK"
      },
      "source": [
        "\n",
        "\n",
        "‚ùì **Questions** ‚ùì \n",
        "\n",
        "1. <u>CNN Architecture and compiler:</u> Create a CNN with your own architecture and a function `load_own_model` that will be able to generate it. Some advice:\n",
        "    - Incorporate the Rescaling Layer in your Sequential architecture\n",
        "    - Add three Conv2D/MaxPooling2D combinations with an increasing number of channels and a decreasing size of kernels for example (be creative, that is not a rule of thumb, mastering CNN is an art)\n",
        "    - Don't forget the Flatten layer and some hidden layers\n",
        "    - Finish with the predictive layer\n",
        "    - Compile your CNN model accordingly\n",
        "  \n",
        "  \n",
        "2. <u>Training and comparison</u>:\n",
        "    - Train your CNN\n",
        "    - Compare its performance to a baseline accuracy\n",
        "\n",
        "<details>\n",
        "    <summary><i>Recommended architecture:</i></summary>\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
        "model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
        "\n",
        "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
        "model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "```\n",
        "\n",
        "        \n",
        "</details>        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "\n",
        "def initialize_model():\n",
        "  model = models.Sequential()\n",
        "\n",
        "# Notice this cool new layer that \"pipe\" your rescaling within the architectur\n",
        "  model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
        "\n",
        "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
        "  model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "  model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
        "  model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "  model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
        "  model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(layers.Dense(3, activation='softmax'))  \n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "    \n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "HOQoGVZSBTrv"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = initialize_model()"
      ],
      "metadata": {
        "id": "a49qY5GKCDFS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "es = EarlyStopping(patience=30, restore_best_weights=True)\n",
        "res = model.fit(X_train, y_train, batch_size=16, \n",
        "          epochs=5,    \n",
        "          validation_split=0.3,\n",
        "          callbacks=[es],\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMNGX7W0DKrp",
        "outputId": "e190f6e8-cfda-45f7-b570-6d186574754a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "25/25 [==============================] - 78s 3s/step - loss: 1.0553 - accuracy: 0.4035 - val_loss: 0.9000 - val_accuracy: 0.4942\n",
            "Epoch 2/5\n",
            "25/25 [==============================] - 72s 3s/step - loss: 0.9295 - accuracy: 0.5088 - val_loss: 1.2061 - val_accuracy: 0.4302\n",
            "Epoch 3/5\n",
            "25/25 [==============================] - 72s 3s/step - loss: 0.9468 - accuracy: 0.5288 - val_loss: 0.8649 - val_accuracy: 0.6337\n",
            "Epoch 4/5\n",
            "25/25 [==============================] - 72s 3s/step - loss: 0.8843 - accuracy: 0.5514 - val_loss: 0.8443 - val_accuracy: 0.5523\n",
            "Epoch 5/5\n",
            "25/25 [==============================] - 72s 3s/step - loss: 0.8536 - accuracy: 0.5789 - val_loss: 0.8197 - val_accuracy: 0.6105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPw9OTWxFl9H",
        "outputId": "6708ee8b-aa1d-446e-9c01-dd3d2ff09004"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 5s 1s/step - loss: 0.8237 - accuracy: 0.6443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eys9hshv6Zo8",
        "outputId": "ffd42e4b-8be1-49c8-953a-5619538abce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy = 64.0 %\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = res[-1]\n",
        "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DC2SFwUZ2vR"
      },
      "source": [
        "ü•° <b><u>Takeaways from building your own CNN</u></b>:\n",
        "* On an \"easy dataset\" like the MNIST, it is now easy to reach a decent accuracy. But for a more complicated problem like classifying flowers, it already becomes more challenging. Take a few minutes to play with the following link before moving on to Transfer Learning\n",
        "    * [PoloClub/CNN-Explainer](https://poloclub.github.io/cnn-explainer/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx8V9sny7cLP"
      },
      "source": [
        "## (5) Using a pre-trained CNN = Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxhI8bluZ2vR"
      },
      "source": [
        "As we said in the beginning, tech companies and university labs have more computational resources than we do.\n",
        "\n",
        "üî• The [**Visual Geometry Group**](https://www.robots.ox.ac.uk/~vgg/data/) *(Oxford University, Department of Science and Engineering)* became famous for some of their **Very Deep Convolutional Neural Networks**: the [**VGG16**](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
        "\n",
        "Take 7 minutes of your time to watch this incredible video of Convolutional Layers created by Dimitri Dmitriev.\n",
        "\n",
        "* üì∫ **[VGG16 Neural Network Visualization](https://www.youtube.com/watch?v=RNnKtNrsrmg)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldqCInZ2vS"
      },
      "source": [
        "### (5.1) Load VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr6m5eKs9s54"
      },
      "source": [
        "‚ùì **Question: loading the VGG16** ‚ùì \n",
        "\n",
        "* Write a first function `load_model()` that loads the pretrained VGG-16 model from `tensorflow.keras.applications.vgg16`. Have a look at the documentation üìö  [tf/keras/applications/VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16)üìö\n",
        "\n",
        "* We will **load the VGG16 model** the following way:\n",
        "    - ü§Ø Let's use the **weights** learned on the [**imagenet dataset**](https://www.image-net.org/download.php) (14M pictures with 20k labels)\n",
        "    - The **`input_shape`** corresponds to the input shape of your images \n",
        "        - Note: *You have to resize them down to a consistent shape if they have different height/widths/channels*\n",
        "    - The **`include_top`** argument should be set to `False`: \n",
        "        - to avoid loading the weights of the fully-connected layers of the VGG16\n",
        "        - and also remove the last layer of the VGG16 which was specifically trained on `imagenet`\n",
        "\n",
        "<i><u>Remark:</u></i> Do not change the default value of the other arguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5X2X8mFIQyr",
        "outputId": "a41ea17f-28ff-4a94-a899-6564375da582"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(571, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "EufUdC0V6Zo8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "def load_model():\n",
        "    \n",
        "    model = VGG16(include_top=False,input_shape=(256, 256, 3))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3psG3JGF7TO"
      },
      "source": [
        "‚ùì **Question: number of parameters in the VGG16** ‚ùì \n",
        "\n",
        "Look at the architecture of the model using ***.summary()***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JtBpKLegF7TO",
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c5a71c-1356-48cc-a6b5-766f51515445"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "model = load_model()\n",
        "model.input_shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfSB8QDsIs5g",
        "outputId": "0f7ed78c-7ed7-4188-df9c-4899edff3287"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IAwxRVF7TO"
      },
      "source": [
        "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
        "\n",
        "üí™ Impressive, right? Two things to notice:\n",
        "- It ends with a combo Conv2D/MaxPooling2D \n",
        "- The `layers.Flatten` and the `layers.Dense` are not there yet, we need to add them.\n",
        "- There are more than 14,000,000 parameters, which is a lot... \n",
        "    - We could fine-tune them, i.e. update them as we will update the weights of the dense layers, but it will take a lot of time....\n",
        "    - For this reason, we will inform the model that the layers before the flattening will be set non-trainable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehMaUN34Z2vT"
      },
      "source": [
        "‚ùì **Question: deactivating the training of the VGG16 paramters** ‚ùì \n",
        "\n",
        "* Write a first function which:\n",
        "    - takes the previous model as the input\n",
        "    - sets the first layers to be non-trainable, by applying **`model.trainable = False`**\n",
        "    - returns the model.\n",
        "\n",
        "* Then inspect the summary of the model to check that the parameters are no longer trainable, they were set to be **`non-trainable`**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "pwdWc_7i6Zo9"
      },
      "outputs": [],
      "source": [
        "def set_nontrainable_layers(model):\n",
        "    \n",
        "    model.trainable = False\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = set_nontrainable_layers(model)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XoBvxg6K2Ze",
        "outputId": "a9cc842c-c9b4-42b9-e083-d35686abc7d0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yedT2VF7TP"
      },
      "source": [
        "‚ùì **Question: chaining the pretrained convolutional layers of VGG16 with our own dense layers** ‚ùì \n",
        "\n",
        "We will write a function that adds flattening and dense layers after the convolutional layers. To do so, we cannot directly use the classic `layers.Sequential()` instantiation.\n",
        "\n",
        "For that reason, we will discover another way here. The idea is that we define each layer (or group of layers) separately. Then, we concatenate them. Have a look at this example: \n",
        "\n",
        "---\n",
        "```python\n",
        "base_model = load_model()\n",
        "base_model = set_nontrainable_layers(base_model)\n",
        "flattening_layer = layers.Flatten()\n",
        "dense_layer = layers.Dense(SOME_NUMBER_1, activation='relu')\n",
        "prediction_layer = layers.Dense(SOME_NUMBER_2, activation='APPROPRIATE_ACTIVATION')\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  flattening_layer,\n",
        "  dense_layer,\n",
        "  prediction_layer\n",
        "])\n",
        "\n",
        "```\n",
        "---\n",
        "\n",
        "* The first line loads a group of layers which is the previous VGG-16 model. \n",
        "* Then, we set these layers to be non-trainable.\n",
        "* Eventually, we can instantiate as many layers as we want.\n",
        "* Finally, we use the `Sequential` with the sequence of layers that will correspond to our overall neural network. \n",
        "\n",
        "Replicate the following steps by adding:\n",
        "* a flattening layer\n",
        "* two dense layers (the first with 500 neurons) to the previous VGG-16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "toQul6-w6Zo-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "def add_last_layers(model):\n",
        "    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n",
        "    base_model = load_model()\n",
        "    base_model = set_nontrainable_layers(base_model)\n",
        "    flattening_layer = layers.Flatten()\n",
        "    dense_layer = layers.Dense(500, activation='relu')\n",
        "    prediction_layer = layers.Dense(3, activation='softmax')\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "      base_model,\n",
        "      flattening_layer,\n",
        "      dense_layer,\n",
        "      prediction_layer\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-24j0psF7TQ"
      },
      "source": [
        "‚ùì **Question: inspect the parameters of a customized VGG16** ‚ùì \n",
        "\n",
        "* Now look at the layers and the parameters of your model. \n",
        "* Note that there is a distinction, at the end, between the **trainable** and **non-trainable parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0uqGuxsJF7TR",
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d09022e-4a99-4ac2-db61-e3aec024cc70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 500)               16384500  \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 1503      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,100,691\n",
            "Trainable params: 16,386,003\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = add_last_layers(model)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxXlnPp5F7TR"
      },
      "source": [
        "‚ùì **Question: building a function that creates a full customized VGG16 and compiles it** ‚ùì \n",
        "\n",
        "* Write a function which builds and compiles your model\n",
        "    * We advise using the _adam_ optimizer with `learning_rate=1e-4`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "T3Oi3Xa86Zo-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def build_model():\n",
        "  adam_opt = optimizers.Adam(learning_rate=1e-4)\n",
        "  model = load_model()\n",
        "  model = set_nontrainable_layers(model)\n",
        "  model = add_last_layers(model)\n",
        "  model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=adam_opt,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S96qsiKxZ2vU"
      },
      "source": [
        "### (5.2) Back to the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkwOw1eF7TS"
      },
      "source": [
        "üö® The VGG16 model was trained on images which were preprocessed in a specific way. This is the reason why we did _NOT_ normalize them earlier.\n",
        "\n",
        "‚ùì **Question: preprocessing the dataset** ‚ùì \n",
        "\n",
        "Apply the specific processing to the original (non-normalized) images here using the method **`preprocess_input`** that you can import from **`tensorflow.keras.applications.vgg16`**\n",
        "\n",
        "üìö Cf. [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "WzzCnpNs6Zo-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "uNeJZvtV3YDf",
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "X_train_proc = preprocess_input(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()"
      ],
      "metadata": {
        "id": "6k3yo-pAO2yv"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hdmcLteuOtWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = model.fit(X_train_proc, y_train, batch_size=16, \n",
        "          epochs=5,    \n",
        "          validation_split=0.3,\n",
        "          callbacks=[es],\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe1zpieVOtpB",
        "outputId": "99a17cf2-974f-4a7d-93ac-0ca6fbc30dde"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "25/25 [==============================] - 376s 15s/step - loss: 2.7914 - accuracy: 0.8020 - val_loss: 2.6348 - val_accuracy: 0.7849\n",
            "Epoch 2/5\n",
            "25/25 [==============================] - 374s 15s/step - loss: 0.2281 - accuracy: 0.9749 - val_loss: 2.9720 - val_accuracy: 0.8605\n",
            "Epoch 3/5\n",
            "25/25 [==============================] - 375s 15s/step - loss: 0.1328 - accuracy: 0.9925 - val_loss: 2.3987 - val_accuracy: 0.8779\n",
            "Epoch 4/5\n",
            "25/25 [==============================] - 382s 15s/step - loss: 6.9871e-05 - accuracy: 1.0000 - val_loss: 2.3631 - val_accuracy: 0.8721\n",
            "Epoch 5/5\n",
            "25/25 [==============================] - 373s 15s/step - loss: 2.0044e-05 - accuracy: 1.0000 - val_loss: 2.3591 - val_accuracy: 0.8721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T7HvbQfZ2vZ"
      },
      "source": [
        "### (5.3)  Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2H0KZF-EoI"
      },
      "source": [
        "\n",
        "\n",
        "‚ùì **Question: Training the customized VGG16** ‚ùì \n",
        "\n",
        "* Train the model with an Early stopping criterion on the validation accuracy -\n",
        "* Since the validation data is provided use `validation_data` instead of `validation_split`.\n",
        "\n",
        "_As usual, store the results of your training into a `history` variable._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grmnNmjeAXcQ",
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_I9JpiAm-W"
      },
      "source": [
        "‚ùì **Question: Looking at the accuracy** ‚ùì \n",
        "\n",
        "Plot the accuracy for both the train set and and the validation set using the usual function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "x9gvV4696Zo_"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
        "    if axs is not None:\n",
        "        ax1, ax2 = axs\n",
        "    else:\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
        "        exp_name = '_' + exp_name\n",
        "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
        "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
        "    #ax1.set_ylim(0., 2.2)\n",
        "    ax1.set_title('loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
        "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
        "    #ax2.set_ylim(0.25, 1.)\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.legend()\n",
        "    return (ax1, ax2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ESzinGOY6aBc",
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "df4a4cfb-ddbd-456a-9461-902d87707079"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<matplotlib.axes._subplots.AxesSubplot at 0x7f50cc71ca10>,\n",
              " <matplotlib.axes._subplots.AxesSubplot at 0x7f50cc70d9d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plot_history(res, title='Accuracy with pre-trained model', exp_name=\"Name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3plexlQAtcC"
      },
      "source": [
        "‚ùì **Question: Evaluating the model** ‚ùì\n",
        "\n",
        "Evaluate the customized VGG16 accuracy on the test set. Did we improve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ps_9HwUyRVj9",
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c19da9-e0b7-41d1-ad07-98cb71e0e6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 104s 21s/step - loss: 2.7809 - accuracy: 0.8456\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = results[-1]\n",
        "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl3aIpsxWb64",
        "outputId": "5eec01cc-2773-4249-ef27-b81cb030c404"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy = 85.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5T1KvsGZ2va"
      },
      "source": [
        "## (6) (Optional) Improve the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF39HIb7BSOy"
      },
      "source": [
        "Now, you can try to improve the model's test accuracy. To do that, here are some options you can consider\n",
        "\n",
        "1. **Unfreeze and finetune**: Source: [Google tutorial](https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning) \n",
        ">_Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate. This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind. It is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features. It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way._\n",
        "\n",
        "\n",
        "1. Add **Data Augmentation** if your model is overfitting. \n",
        "\n",
        "2. If your model is not overfitting, try a more complex model.\n",
        "\n",
        "3. Perform a precise **Grid Search** on all the hyper-parameters: learning_rate, batch_size, data augmentation etc...\n",
        "\n",
        "4. **Change the base model** to more modern one CNN (ResNet, EfficientNet1,... available in the keras library)\n",
        "\n",
        "5. Curate the data: maintaining a sane data set is one of the keys to success.\n",
        "\n",
        "6. Collect more data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3UMNBZHZ2vb"
      },
      "source": [
        "## (6.2) Comparing the performances of the CNN, the VGG, and the VGG trained on the augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PQLnU_H6Zo_"
      },
      "outputs": [],
      "source": [
        "test_accuracy_aug = res_aug[-1]\n",
        "\n",
        "\n",
        "print(f\"test_accuracy_aug = {round(test_accuracy_aug,2)*100} %\")\n",
        "\n",
        "print(f\"test_accuracy_vgg = {round(test_accuracy_vgg,2)*100} %\")\n",
        "\n",
        "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")\n",
        "\n",
        "print(f'Chance level: {1./num_classes*100:.1f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gaSAxLZ2vc"
      },
      "source": [
        "---\n",
        "\n",
        "üèÅ **Congratulations** üèÅ \n",
        "\n",
        "1. Download this notebook from your `Google Drive` or directly from `Google Colab` \n",
        "2. Drag-and-drop it from your `Downloads` folder to your local challenge folder  \n",
        "\n",
        "\n",
        "üíæ Don't forget to push your code\n",
        "\n",
        "3. Follow the usual procedure on your terminal inside the challenge folder:\n",
        "      * *git add cifar_classification.ipynb*\n",
        "      * *git commit -m \"I am the god of Transfer Learning\"*\n",
        "      * *git push origin master*\n",
        "\n",
        "*Hint*: To find where this Colab notebook has been saved, click on `File` $\\rightarrow$ `Locate in Drive`.\n",
        "\n",
        "üöÄ If you have time, move on to the **Autoencoders** challenge!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}